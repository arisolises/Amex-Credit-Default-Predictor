{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07b4e42",
   "metadata": {},
   "source": [
    "# AMEX: Credit Default Predictor  \n",
    "\n",
    "Description, Bussiness Problem, Data Explanation and ETL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241533d",
   "metadata": {},
   "source": [
    "\n",
    "- Business Problem:\n",
    "The business problem is credit default prediction, which is essential for managing risk in consumer lending businesses. Lenders need accurate models to assess the creditworthiness of potential borrowers and make informed lending decisions.\n",
    "\n",
    "\n",
    "- Business Context:\n",
    "American Express, the largest payment card issuer globally, is hosting a competition to explore improved credit default prediction models. They aim to enhance the customer experience by making it easier for cardholders to be approved for a credit card. By leveraging machine learning techniques and an industrial-scale dataset, participants will challenge the existing credit default prediction model used by American Express.\n",
    "\n",
    "- Main Goal:\n",
    "Develop a machine learning model that surpasses the current credit default prediction model used by American Express, improving accuracy and performance metrics to enhance the customer experience and optimize lending decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391f9d8",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089c5cb",
   "metadata": {},
   "source": [
    "### Extract, Tranform, Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578cd326",
   "metadata": {},
   "source": [
    "\n",
    "The datasets can be found on Kaggle, and we will utilize the Kaggle API to download them.\n",
    "\n",
    "There are four distinct datasets available: train data, test data, train labels, and submission data.\n",
    "\n",
    "Before delving into the project, it is crucial to review the sizes of these datasets to gain an understanding of the dataset's magnitude we are working with.\n",
    "\n",
    "\n",
    "- Submission data: 61.95 mb\n",
    "- Train labels data: 30.75 mb\n",
    "- Train data: 16.39 gb\n",
    "- Test data: 32.82 gb\n",
    "\n",
    "\n",
    "\n",
    "Upon reviewing the sizes of the datasets, we discovered that two of them are quite large, approximately 16 and 32 GB in size. This indicates the potential need for techniques and technologies capable of handling large datasets. Whether it's during the ETL (Extract, Transform, Load) process, data analysis, or model training, the large dataset size can pose challenges.\n",
    "\n",
    "For the current phase, our plan is to work with JupyterLab locally on a MacBook Pro equipped with an M1 Pro chip. \n",
    "\n",
    "Will be explained later in case a computational enhancer is required. \n",
    "\n",
    "\n",
    "Lets start with the code to extract the data locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2eb65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/arielsolis/.kaggle/kaggle.json'\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/arielsolis/.kaggle/kaggle.json'\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/arielsolis/.kaggle/kaggle.json'\n",
      "amex-default-prediction.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "def kaggle_downloads(file, final_path):\n",
    "    # Please create an account and download the kaggle.json with your credentials. \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # Download the file\n",
    "    api.dataset_download_file(file, path=final_path, quiet=False)\n",
    "\n",
    "    # Unzip file\n",
    "    extension = os.path.splitext(file)[1]\n",
    "    if extension == \".zip\":\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(os.path.join(final_path, file), \"r\") as zip_ref:\n",
    "            zip_ref.extractall(final_path)\n",
    "        os.remove(os.path.join(final_path, file))\n",
    "\n",
    "    print(\"File downloaded correctly.\")\n",
    "\n",
    "# Use the function\n",
    "file_name = \"amex-default-prediction\n",
    "path= \"/Users/arielsolis\"\n",
    "\n",
    "kaggle_downloads(nombre_archivo, ruta_destino)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b4a8f",
   "metadata": {},
   "source": [
    "It is crucial to note that if you are utilizing MacOS, the \"kaggle.json\" file must be saved in the directory that the notebook is located in, followed by **\"/.kaggle\"**. This directory path ensures that the Kaggle API can access the necessary credentials for downloading the datasets.  \n",
    "\n",
    "\n",
    "Due to the large size of the datasets, it is expected that compiling them may take a considerable amount of time. However, once the datasets are compiled, we can proceed to utilize specific packages or libraries to handle the data efficiently.\n",
    "\n",
    "\n",
    "Lets start using pandas for train data and train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de765a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#Load train and train labels datasets\n",
    "train_labels = pd.read_csv('train_labels.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eebdd1",
   "metadata": {},
   "source": [
    "Indeed, Pandas has efficient capabilities for loading datasets quickly, even for larger datasets.\n",
    "\n",
    "\n",
    "Certainly! In cases where the local storage and computational efficiency become limiting factors, it is a wise choice to leverage Apache Spark for handling the train data and test data instead of relying solely on Pandas.\n",
    "\n",
    "Apache Spark is a powerful distributed computing framework designed to process and analyze large-scale datasets efficiently. Here's why using Spark is beneficial in such scenarios:\n",
    "multiple machines or cores. This distributed processing capability significantly enhances the performance and scalability of data operations, making it ideal for handling large datasets.\n",
    "\n",
    "- Fault Tolerance: PySpark incorporates fault tolerance mechanisms, ensuring that data processing tasks can recover from failures. It automatically handles data replication and re-executes failed tasks, ensuring the reliability and stability of data processing workflows.\n",
    "\n",
    "- Scalability: As the dataset size increases, PySpark can easily scale up by adding more computational resources. It can efficiently handle datasets that surpass the memory limits of a single machine by leveraging distributed computing resources.\n",
    "\n",
    "- Built-in Data Processing Functions: PySpark provides a wide range of built-in functions for data processing, transformation, and analysis. It offers a familiar DataFrame API similar to Pandas, making it convenient for data manipulation tasks.\n",
    "\n",
    "- Compatibility: PySpark integrates well with other big data technologies and platforms, such as Hadoop and Spark clusters. It supports various data sources, including CSV, Parquet, and more, allowing seamless integration with different data formats.\n",
    "\n",
    "By using PySpark locally, we can leverage its distributed computing capabilities, fault tolerance mechanisms, and scalability features to handle and process the test data efficiently, even when dealing with large datasets that exceed the capacity of a single machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ecac863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/23 19:44:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/23 19:44:55 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Build a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100\")\\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262fb689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/23 19:14:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.csv('train_data.csv', header=True,inferSchema=True)\n",
    "\n",
    "\n",
    "# Specify the output path for the Parquet file\n",
    "output_path = 'train_data.parquet'\n",
    "\n",
    "# Write the DataFrame to Parquet format\n",
    "spark_df.write.parquet(output_path)\n",
    "spark_df.coalesce(1)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a99f45",
   "metadata": {},
   "source": [
    "Certainly! From the previous code, there are two notable aspects. Firstly, we are converting the CSV file to the Parquet format, and secondly, the resulting output is a folder containing the dataset divided into partitions. Let's explore the reasons behind these choices and address concatenating the partitions into a single dataset.\n",
    "\n",
    "Conversion to Parquet:\n",
    "\n",
    "- Improved Performance: Parquet is a columnar storage format that offers efficient compression and encoding techniques. By converting the dataset to Parquet, we can optimize storage and achieve faster query performance due to its columnar layout.\n",
    "- Compatibility: Parquet is widely supported in various data processing frameworks, including Apache Spark and Pandas. It provides interoperability between different tools, allowing seamless integration and data sharing.\n",
    "\n",
    "\n",
    "The result of this code will be a folder with the dataset in partitions. Let's try to concatenate all these partitions to have only one dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20497c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# File\n",
    "folder = '/Users/arielsolis/train_data.parquet'\n",
    "\n",
    "# List of  files\n",
    "files_parquet = [file for file in os.listdir(folder) if file.endswith('.parquet')]\n",
    "\n",
    "# Read and cocatenate those files\n",
    "dfs = []\n",
    "for file in files_parquet:\n",
    "    file_path = os.path.join(folder, file)\n",
    "    table = pq.read_table(file_path)\n",
    "    df = table.to_pandas()\n",
    "    dfs.append(df)\n",
    "    \n",
    "    \n",
    "# Concatenate DataFrames\n",
    "train = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "train.to_parquet('/Users/arielsolis/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78eb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd2306",
   "metadata": {},
   "source": [
    "Now that that we load the train dataset lets merge the train and target dataset. We dont know if the target dataset is sort as same as the train dataset so its better to merge both with a left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c2eaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both datasets have 5531451 records\n",
      "Target column without null values\n",
      "the Target column has [0 1] unique values\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets\n",
    "train_merge = train.merge(train_labels,how='left',on='customer_ID')\n",
    "\n",
    "# This is and important step a simple validation of the new dataset\n",
    "\n",
    "# Validate same number of rows\n",
    "if train.shape[0] == train_merge.shape[0]:\n",
    "   print(f'Both datasets have {train_merge.shape[0]} records')\n",
    "else:\n",
    "    print('Not the same amount of records')\n",
    "    \n",
    "# Validate the target no null values and just two values [0,1]\n",
    "\n",
    "if train_merge.target.isna().sum() ==0:\n",
    "    \n",
    "    print('Target column without null values')\n",
    "    print(f'the Target column has {train_merge.target.unique()} unique values')\n",
    "else:\n",
    "    print('Target with null values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd8c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merge.to_parquet('train_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812aad7d",
   "metadata": {},
   "source": [
    "\n",
    "In the case of the test data set, lets preprocess as same as the train dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db433c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/23 19:46:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.csv('test_data.csv', header=True,inferSchema=True)\n",
    "\n",
    "\n",
    "# Specify the output path for the Parquet file\n",
    "output_path = 'test_data.parquet'\n",
    "\n",
    "# Write the DataFrame to Parquet format\n",
    "spark_df.write.parquet(output_path)\n",
    "spark_df.coalesce(1)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5981546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# File\n",
    "folder = '/Users/arielsolis/test_data.parquet'\n",
    "\n",
    "# List of  files\n",
    "files_parquet = [file for file in os.listdir(folder) if file.endswith('.parquet')]\n",
    "\n",
    "# Read and cocatenate those files\n",
    "dfs = []\n",
    "for file in files_parquet:\n",
    "    file_path = os.path.join(folder, file)\n",
    "    table = pq.read_table(file_path)\n",
    "    df = table.to_pandas()\n",
    "    dfs.append(df)\n",
    "    \n",
    "    \n",
    "# Concatenate DataFrames\n",
    "test = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63930ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "test.to_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406b3ba",
   "metadata": {},
   "source": [
    "Now with oall that preprocess lets check the size of the datasets:\n",
    "\n",
    "- Train data: 6.68 gb\n",
    "- Test data: 13.7 gb\n",
    "\n",
    "50% and 58% of size reduce!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c7323",
   "metadata": {},
   "source": [
    "After obtaining the datasets in the desired format, it is recommended to utilize a solution for storing these datasets beyond just the local environment. In this case, we will employ Amazon S3 buckets from AWS as the storage solution. However, it's important to note that the choice of using AWS S3 is based on a simple preference, and other cloud storage options or even personal drives could be used as well.\n",
    "\n",
    "Here are the reasons why using AWS S3 buckets is a popular choice:\n",
    "\n",
    "- Scalability: AWS S3 offers virtually unlimited scalability, allowing you to store and manage large volumes of data without worrying about storage capacity limitations. It can handle datasets of any size, providing a flexible and scalable storage solution.\n",
    "\n",
    "- Durability and Reliability: S3 ensures high durability and reliability for your data. It replicates data across multiple availability zones, providing redundancy and protecting against data loss. It also offers versioning and backup features, ensuring the safety and integrity of your datasets.\n",
    "\n",
    "- Accessibility and Availability: AWS S3 provides easy and secure access to your data from anywhere at any time. It offers fine-grained access controls, enabling you to manage permissions and share data with specific users or services. Additionally, S3 provides high availability, ensuring that your datasets are accessible without downtime or interruptions.\n",
    "\n",
    "- Integration and Ecosystem: AWS S3 integrates seamlessly with a wide range of AWS services, such as data processing frameworks (e.g., Apache Spark), machine learning platforms (e.g., Amazon SageMaker), and analytics tools. It forms part of the robust AWS ecosystem, offering various data management and analysis capabilities.\n",
    "\n",
    "While AWS S3 is chosen here as a preferred storage solution, it's important to note that other cloud storage options like Google Cloud Storage, Microsoft Azure Blob Storage, or personal drives such as Google Drive or Dropbox can also be used based on individual preferences, requirements, and existing infrastructure.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51117621",
   "metadata": {},
   "source": [
    "### AWS Storage (s3 buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2b2da",
   "metadata": {},
   "source": [
    "\n",
    "Indeed, before utilizing AWS products and services, it is essential to create an AWS account. Here are the steps to create an AWS account:\n",
    "\n",
    "- Visit the AWS website: Go to the official AWS website at https://aws.amazon.com/.\n",
    "\n",
    "Once you have the account we can start working on AWS services!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44aaa351-372d-44e6-bead-d0a80abd1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3 # AWS package\n",
    "\n",
    "# You need credentials to be able to use python to create, load or download anything in AWS services\n",
    "\n",
    "ACCESS_KEY = 'AKIAYESKYKNUCG5RIZ5J'  \n",
    "SECRET_KEY = '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'\n",
    "\n",
    "BUCKET_NAME = 'amex-dfs' # Define a bucket name\n",
    "REGION = 'us-east-2'  # Specify the desired region for the bucket\n",
    "location_constraint = 'us-east-2' # You have to use same location as REGION\n",
    "s3 = boto3.client('s3',region_name=REGION, aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0005267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'M7EB2VA00GWKJA7Z',\n",
       "  'HostId': 'mOjlUwxmz1++pqDbLz/P390907FafbRDi7AF/Hp3f+aBI5l0OlsGtK7RvDPZRLBj4OsjM59JocA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'mOjlUwxmz1++pqDbLz/P390907FafbRDi7AF/Hp3f+aBI5l0OlsGtK7RvDPZRLBj4OsjM59JocA=',\n",
       "   'x-amz-request-id': 'M7EB2VA00GWKJA7Z',\n",
       "   'date': 'Wed, 24 May 2023 00:37:34 GMT',\n",
       "   'location': 'http://amex-dfs.s3.amazonaws.com/',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Location': 'http://amex-dfs.s3.amazonaws.com/'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create the S3 bucket\n",
    "s3.create_bucket(Bucket=BUCKET_NAME, CreateBucketConfiguration={'LocationConstraint': location_constraint})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f762f8",
   "metadata": {},
   "source": [
    "After creating the s3 bucket we can add the datasets to a folder, also just in case the test dataset is to large we are going to add the folder with the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('test.parquet', BUCKET_NAME, 'test_data.parquet')\n",
    "s3.upload_file('train_dataset.parquet', BUCKET_NAME, 'train_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f91cc5",
   "metadata": {},
   "source": [
    "You have to be patient, this case would depend on your internet velocity so uploading the files into s3 could take some time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc31bd",
   "metadata": {},
   "source": [
    "## SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc9c7e",
   "metadata": {},
   "source": [
    "- To connect locally, the Kaggle API is used, and it is crucial to save the \"kaggle.json\" file in the correct path.\n",
    "\n",
    "- Once the datasets are downloaded, they can be loaded using Python with both Spark and Pandas. Spark is preferred for handling heavy files due to its distributed computing capabilities.\n",
    "\n",
    "- The datasets can be transformed to the Parquet format using Spark, which provides efficient storage and optimized query performance. The goal is to create a single unpartitioned DataFrame for further processing.\n",
    "\n",
    "- Next, the train dataset can be joined with the target dataset, ensuring that there are no errors or mismatches in the join operation. This step helps validate the integrity and consistency of the data.\n",
    "\n",
    "- To store the transformed files remotely and access them without relying on local storage, an AWS account is created. The transformed datasets can be saved using AWS S3, which provides scalable and durable object storage in the cloud. This enables remote access to the datasets and reduces the reliance on local storage infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fb86c",
   "metadata": {},
   "source": [
    "Thanks,\n",
    "\n",
    "Ariel Solis "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
